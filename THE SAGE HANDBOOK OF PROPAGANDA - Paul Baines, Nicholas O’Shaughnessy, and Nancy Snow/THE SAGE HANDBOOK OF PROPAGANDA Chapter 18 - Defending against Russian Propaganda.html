<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>THE SAGE HANDBOOK OF PROPAGANDA</title><meta name="author" content="Jacob"/><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 .s1 { color: #FFF; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 57.5pt; }
 .s2 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 32.5pt; }
 .s3 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 13.5pt; }
 h1 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12.5pt; }
 p { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11.5pt; margin:0pt; }
 .s4 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; }
 .h2 { color: black; font-family:Times, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11.5pt; }
 .s5 { color: black; font-family:Times, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11.5pt; }
 .s6 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10.5pt; }
 .a { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; }
 li {display: block; }
 #l1 {padding-left: 0pt; }
 #l1> li>*:first-child:before {content: "• "; color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; }
 li {display: block; }
 #l2 {padding-left: 0pt; }
 #l2> li>*:first-child:before {content: "• "; color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; }
</style></head><body><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="719" height="166" alt="image" src="THE SAGE HANDBOOK OF PROPAGANDA Chapter 18 - Defending against Russian Propaganda/Image_001.gif"/></span></p><p class="s1" style="padding-top: 41pt;text-indent: 0pt;text-align: right;">18</p><p style="text-indent: 0pt;text-align: left;"/><p class="s2" style="padding-top: 4pt;padding-left: 52pt;text-indent: 0pt;line-height: 37pt;text-align: right;">Defending against Russian</p><p class="s2" style="padding-left: 52pt;text-indent: 0pt;line-height: 37pt;text-align: right;">Propaganda</p><p class="s3" style="padding-top: 29pt;padding-left: 158pt;text-indent: 0pt;text-align: left;">Christopher Paul and Miriam Matthews </p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-top: 5pt;padding-left: 52pt;text-indent: 0pt;text-align: left;">INTRODUCTION</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 52pt;text-indent: 0pt;text-align: right;">This chapter discusses Russian propaganda, explains why it might work, and explores options available for Western countries to protect themselves from manipulation through propaganda. In previous research, we charac- terized the Russian approach to propaganda as a ‘firehose of falsehood’, capturing its volume, frequency, and lack of commitment to objective reality (Paul &amp; Matthews 2016). The discussion here begins with a description of the nature and character of Russian propa- ganda, Russia’s various techniques, their goals, and several examples of specific propa- ganda efforts as reported by news sources and other observers. This discussion should pro- vide you, the reader, with a better understand- ing of the scope and scale of Russia’s efforts. Why is this firehose of falsehood effec- tive? We then turn to experimental results from psychology and social psychology that match characteristics of Russian propaganda with human psychological vulnerabilities</p><p style="padding-top: 4pt;padding-left: 12pt;text-indent: 0pt;text-align: right;">and limitations. This research reveals that falsehood-based attempts to manipulate peo- ple through propaganda are far more likely to be successful than we might realize or prefer. What might we do about it? The chapter concludes with a review of various proposals and suggestions for defending against propa- ganda. We evaluate each of these against evi-</p><p style="padding-left: 18pt;text-indent: 0pt;text-align: justify;">dence and practical considerations.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-top: 11pt;padding-left: 18pt;text-indent: 0pt;text-align: justify;">THE CHARACTER OF RUSSIAN PROPAGANDA</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 18pt;text-indent: 0pt;text-align: justify;">In many ways, the current Russian approach to propaganda builds on Soviet Cold War-era techniques with an emphasis on obfuscation and getting targets to act in the interests of the propagandist without realizing that they have done so (Oliker 2015). The Soviets would routinely employ ‘active measures’, a term that encompassed disinformation, for- gery, and subversion (Averin 2018). However,</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 11pt;padding-left: 58pt;text-indent: 0pt;text-align: justify;">these old models are much better suited to the contemporary global information envi- ronment than they were to the level of com- munications technology available during the Cold War. Russia has taken advantage of the technology and media available in the con- temporary context in ways that would have been inconceivable during the Cold War.</p><p style="padding-left: 58pt;text-indent: 11pt;text-align: justify;">While most Western observers focus on Russia’s use of propaganda outside Russia’s own borders, other countries are not the pri- mary targets. Russia uses propaganda to mobilize internal opposition in other coun- tries, but also to mobilize Russia’s own public (Andriukaitis 2018). Thus, Russia produces as much, if not more, propaganda aimed at its own domestic audiences. Much of this inter- nal propaganda seeks to divide Russia from the rest of the world and create the percep- tion that Russia is being besieged on all sides by enemies. This makes it easier for Russian leaders to invoke support for aggressive action in the name of defending the moth- erland and to silence domestic resistance as being unpatriotic (Snyder 2018). The focus of this chapter, however, is on Russian inter- national propaganda. Our primary concern is finding ways to protect others from Russia, not Russians from their own government.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-top: 8pt;padding-left: 58pt;text-indent: 0pt;text-align: left;">SOURCES AND TYPES OF RUSSIAN PROPAGANDA</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 58pt;text-indent: 0pt;text-align: justify;">Russian propaganda includes text, video, audio, and still imagery propagated via tele- vision broadcasting, satellite television, tra- ditional radio, and the Internet and social media. The producers and disseminators include a substantial force of paid internet ‘trolls’ who manage dozens of false online personas and amplify Russian propaganda themes through online chat rooms, discus- sion forums, and comments sections on news and other websites (Chen 2015, Pomerantsev &amp; Weiss 2014). These various media and modes are discussed below.</p><p style="padding-top: 11pt;padding-left: 18pt;text-indent: 11pt;text-align: justify;">Television is a staple for Russian propa- ganda, both traditional broadcast TV and sat- ellite or cable dissemination, echoed online through station websites and video sharing platforms such as YouTube. RT (formerly Russia Today) is one of Russia’s primary mul- timedia news providers. With a budget of more than US$300 million per year, it broadcasts in English, French, German, Spanish, Russian, and several Eastern European languages. The channel is particularly popular online, where it claims more than a billion page views. If true, that would make it the most-watched news source on the Internet (Pomerantsev &amp; Weiss 2014). RT and Sputnik (another self- styled ‘news’ station) project a mixture of actual journalism, infotainment (feel good and human-interest stories), and lightly spun anti-Western stories that highlight shortcom- ings and perceived hypocrisies in the West, such as corruption, abuse of power, or infra- structure failures (Lucas &amp; Nimmo 2015). While some RT content is good journalism and some is spun to be selectively critical of the West, some is unambiguously designed to mislead or obfuscate. Consider, for example, the period immediately following the 2014 shootdown of Malaysia Airlines Flight 17 over Crimea: RT broadcasted more than six different possible explanations for the shoot- down (some plausible, some less so; Snyder 2018), with manufactured evidence support- ing more than one, but never presenting the actual explanation (a Russian-made missile fired by pro-Russian Ukrainian separatists; Thomas 2015).</p><p style="padding-left: 18pt;text-indent: 11pt;text-align: justify;">Russian international broadcasting also includes Russian-language broadcasting in Eastern European countries with signifi- cant Russian-speaking populations. Russia has bought available TV and radio stations throughout this region over the past decade, so it can easily control content and format. Russian programming has high-production values and is generally entertaining, so it is preferred to genuinely local Russian- language programming, which is ‘dry and unattractive’ (Lucas &amp; Nimmo 2015:7).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 11pt;padding-left: 52pt;text-indent: 0pt;text-align: justify;">Viewers tune in to see flashy and entertaining shows; they then stay tuned for ‘news’ that is sometimes heavily laced with propaganda and spin.</p><p style="padding-left: 52pt;text-indent: 11pt;text-align: justify;">The internet is a heavy focus for Russian propaganda. In addition to online echoes of Russian international broadcasting, the inter- net is infested with Russian trolls (fraudulent online accounts operated by humans) and bots (accounts operated by automated pro- cesses; Averin 2018). Volchek and Sindelar (2015) report that ‘thousands of fake accounts on Twitter, Facebook, LiveJournal, and vKontakte’ are maintained by Russian propagandists. The 2018 indictment of the Russian ‘Internet Research Agency’ (a cen- tralized structure for organizing, paying, and tasking trolls) by the US District Court for the District of Columbia revealed several fea- tures of the efforts of this infamous Russian ‘troll factory’, including:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l1"><li><p class="s4" style="padding-left: 67pt;text-indent: -15pt;text-align: justify;">Employing hundreds of individuals to manage fake personas, with an annual budget of millions of US dollars;</p></li><li><p class="s4" style="padding-left: 67pt;text-indent: -15pt;text-align: justify;">Attracting US audiences using false personas and posing as Americans;</p></li><li><p class="s4" style="padding-left: 67pt;text-indent: -15pt;text-align: justify;">Disparaging political candidates prior to the 2016 US election, buying political advertisements (again posing as Americans), staging political rallies, pretending to be American grassroots organizations;</p></li><li><p class="s4" style="padding-left: 67pt;text-indent: -15pt;text-align: justify;">Promoting allegations of vote fraud through personas and groups on social media, as well as through ad buys.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 52pt;text-indent: 0pt;text-align: justify;">It is worth noting that the Internet Research Agency is not the only source of Russian trolls, just one that has been exposed and documented (and criminally indicted). It is entirely possible that Russia maintains other such troll factories, as well as relying on enti- ties less directly tied to the state, such as trolls paid and coordinated by criminal oli- garchs or collectives of patriotic hackers.</p><p style="padding-left: 52pt;text-indent: 11pt;text-align: justify;">Sometimes, Russian propaganda is picked up and rebroadcast by legitimate news outlets; more frequently, innocent social media users repeat the themes, messages,</p><p style="padding-top: 11pt;padding-left: 18pt;text-indent: 0pt;text-align: justify;">or falsehoods introduced by one of Russia’s many dissemination channels (Lucas &amp; Nimmo 2015). For example, German news sources rebroadcast Russian disinformation about atrocities in Ukraine in early 2014 (Lelich 2014), and Russian disinformation about European Union plans to deny visas to young Ukrainian men was repeated with such frequency in Ukrainian media that the Ukrainian general staff felt compelled to post a rebuttal (Goble 2015).</p><p style="padding-left: 18pt;text-indent: 11pt;text-align: justify;">As evidenced by the above descriptions, Russian propaganda has relied on manufac- tured evidence. This fabricated information is often photographic. Some of these images are easily exposed as fake due to poor photo editing, such as discrepancies of scale, or the availability of the original (pre-altered) image (Davis 2014). Russian propagandists have been caught hiring actors to portray vic- tims of manufactured atrocities or crimes for news reports (as was the case when Viktoria Schmidt pretended to have been attacked by Syrian refugees in Germany for Russian’s Zvezda TV network) and faking on-scene news reporting (as shown in a leaked video in which ‘reporter’ Maria Katasonova is revealed to be in a darkened room with recorded explosion sounds playing in the background rather than on a battlefield in Donetsk when a light is switched on during the recording; Smith 2015).</p><p style="padding-left: 18pt;text-indent: 11pt;text-align: justify;">In addition to manufacturing information, Russian propagandists often manufacture sources. Russian news channels such as RT and Sputnik, as well as other forms of media, misquote credible sources or claim a more credible source as the origin of a selected falsehood (Miller 2013). Similarly, several scholars and journalists, including Edward Lucas, Luke Harding, and Don Jensen, have reported that books that they did not write – and containing views clearly contrary to their own – had been published in Russian under their names (Lucas 2015).</p><p style="padding-left: 18pt;text-indent: 11pt;text-align: justify;">Using these different modes and media, general Russian tactics have been described</p><p style="padding-left: 18pt;text-indent: 0pt;line-height: 15pt;text-align: justify;">as efforts to perpetrate the ‘four Ds’: <span class="h2">dismiss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 58pt;text-indent: 0pt;line-height: 71%;text-align: justify;">the critic, <span class="h2">distort </span>the facts, <span class="h2">distract </span>from the issue, and <span class="h2">dismay </span>the audience (Lucas</p><p style="padding-left: 58pt;text-indent: 0pt;text-align: justify;">&amp; Nimmo 2015:5). Thus, Russia is not just spreading false stories, but trying to sow con- fusion about or distract from truths shared by other sources. Examples include the deni- als of the presence of ‘little green men’ in Crimea in 2014, or the aforementioned inter- pretations offered for the MH-17 shootdown (Thomas 2015).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-top: 8pt;padding-left: 58pt;text-indent: 0pt;text-align: justify;">GOALS AND OBJECTIVES OF RUSSIAN PROPAGANDA</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 58pt;text-indent: 0pt;text-align: justify;">Various scholars and observers have imputed a range of goals, objectives, and motives to Russia in its use of propaganda. All are plau- sible, and none are mutually exclusive. For example, Brooking and Singer (2016) note two broad objectives for Russian propa- ganda: to overwhelm Russia’s adversaries with misinformation, challenging the very basis of their reality; and to mobilize and maintain the support of their own citizens. Lucas and Nimmo (2015) note that Russian propaganda is often less about winning fac- tual arguments and more about spreading confusion. Matthew Armstrong (2014) has described this as a ‘war on information’, seeking to destroy trust in and credibility of all sources of information. Overall, this nihil- istic goal of weakening credibility in general and sowing chaos and discord in the West is a common theme in goals imputed to Russia: ‘Sometimes, the goal is simply to stack tinder, throw matches, and see what happens’ (Brooking &amp; Singer 2016:22). This is con- sistent with Freedom House (2017) reporting on Russian efforts to use propaganda to influence elections in at least 18 countries in 2016 and 2017.</p><p style="padding-left: 58pt;text-indent: 11pt;text-align: justify;">Beyond these broad goals, McGeehan (2018) asserts that Russia seeks to achieve its political and military objectives without escalating to military confrontation. One path to undercutting resistance to Russian</p><p style="padding-top: 5pt;padding-left: 18pt;text-indent: 0pt;text-align: justify;">aims is to manipulate foreign opinion to be sympathetic toward Russian objectives, since in Western democracies, the people are the ultimate decision-makers.</p><p style="padding-left: 18pt;text-indent: 11pt;text-align: justify;">Russian propaganda can also seek narrow, specific goals. For example, in 2016, a small protest outside Incirlik Air Base in Turkey was portrayed as a much larger demonstra- tion as part of a campaign to try to undermine US-Turkish relations (Brooking &amp; Singer 2016).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-top: 8pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">A NOTE ON THE EFFECTIVENESS OF RUSSIAN PROPAGANDA</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 18pt;text-indent: 0pt;text-align: justify;">Just how effective has Russian propaganda been? That is difficult to quantify. Measuring the success of an influence effort requires clearly articulated goals and measurement both before and after the campaign, among other things (Paul et al. 2015). This is difficult because our understanding of Russian goals is partially speculative, and even where we have high confidence in their general goals, we lack specificity about their intended targets; we also lack clear baselines against which to measure change. Research on the effective- ness of Russia’s efforts to date is possible, but it is difficult and little has been done in this regard. Such measurement should be a prior- ity going forward (Applebaum et al. 2018).</p><p style="padding-left: 18pt;text-indent: 11pt;text-align: justify;">Peisakhin and Rozenas (2018) have used Russian-language television broadcast foot- prints to form a natural experiment to study the influence of Russian propaganda on Ukrainian voters. They found that Russian propaganda was most effective on those who were already favorably disposed toward Russia while hav- ing no or negative effects on anti-Russia Ukrainians. Further, they found that Russian propaganda contributed to increasing political polarization in Ukraine, a concerning finding if it proves to generalize to other countries sub- jected to Russian propaganda. While we do not have good general assessments of the effec- tiveness of Russian propaganda, we can put it</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 11pt;padding-left: 52pt;text-indent: 0pt;text-align: justify;">between a left and right bounds: Russian prop- aganda is more effective than we in the West would prefer that it be, and is less effective than they (the Russians) would like. Further research can only help narrow that bounds.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 52pt;text-indent: 0pt;text-align: left;">THE DISTINCTIVE CHARACTERISTICS OF THE RUSSIAN FIREHOSE OF FALSEHOOD PROPAGANDA MODEL</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 52pt;text-indent: 0pt;text-align: justify;">Based on our observations of Russia’s propa- ganda efforts, we have identified four central characteristics of their approach. First, Russian</p><p style="padding-left: 52pt;text-indent: 0pt;line-height: 15pt;text-align: left;">propaganda is <span class="h2">high volume and multi-channel</span>.</p><p style="padding-left: 52pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">As noted, Russia uses numerous modes, and</p><p style="padding-left: 52pt;text-indent: 0pt;text-align: justify;">has multiple channels (in the broadest possible conception of channels) in each mode. Russia does not just have one international broadcast- ing arm, but several (and more that are Russian funded but not clearly attributed). The trolls of the internet research agency each managed dozens of false personas.</p><p style="padding-left: 52pt;text-indent: 11pt;line-height: 71%;text-align: justify;">Second, Russian propaganda is <span class="h2">rapid, continuous, and repetitive</span>. Contemporary</p><p style="padding-left: 52pt;text-indent: 0pt;text-align: justify;">Russian propaganda is continuous and very responsive to events. Due to their willingness to perpetuate falsehoods, Russian propagan- dists do not need to wait to check facts or verify claims; they just disseminate an inter- pretation of emergent events that appears to best favor their themes and objectives. This allows them to be remarkably responsive and nimble, often broadcasting the first ‘news’ of events (and, with similar frequency, the first news of non-events, or things that have not actually happened). They will also repeat and recycle disinformation. The January 14, 2016,</p><p style="padding-left: 52pt;text-indent: 0pt;line-height: 15pt;text-align: left;">edition of <span class="s5">Weekly Disinformation Review</span></p><p style="padding-left: 52pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">reported the reemergence of several previ-</p><p style="padding-left: 52pt;text-indent: 0pt;text-align: justify;">ously debunked Russian propaganda stories, including that Polish President Andrzej Duda was insisting that Ukraine return former Polish territory, that Islamic State fighters were joining pro-Ukrainian forces, and that there was a Western-backed coup in Kiev, Ukraine’s capital (Disinformation 2016).</p><p style="padding-top: 11pt;padding-left: 18pt;text-indent: 11pt;line-height: 71%;text-align: justify;">Third, Russian propaganda makes <span class="h2">no com- mitment to objective reality</span>. Contemporary</p><p style="padding-left: 18pt;text-indent: 0pt;text-align: justify;">Russian propaganda makes little or no com- mitment to the truth. This is not to say that all of it is false. Quite the contrary: It often contains a significant fraction of the truth but is spun as a selective truth. Sometimes, how- ever, events reported in Russian propaganda are wholly manufactured, as described above. Wardle (2017) notes seven different types of disinformation, and the Russians employ them all: satire or parody, false connection (when the images or headlines do not match the content), misleading content, false context (genuine content but out of context), imposter content (impersonating a genuine source), manipu- lated content (genuine information or imagery that is then changed), and fabricated content.</p><p style="padding-top: 1pt;padding-left: 18pt;text-indent: 11pt;line-height: 85%;text-align: justify;">Fourth, and finally, Russian propaganda makes <span class="h2">no commitment to consistency</span>. Different Russian media do not necessarily</p><p style="padding-left: 18pt;text-indent: 0pt;text-align: justify;">broadcast the exact same themes or mes- sages. Different channels do not necessar- ily broadcast the same account of contested events. Different channels or representatives show no fear of ‘changing their tune’. If one falsehood or misrepresentation is exposed or is not well received, the propagandists will discard it and move on to a new (though not necessarily more plausible) explana- tion (Snyder 2018). Lack of commitment to consistency extends to the statements of Russian President Vladimir Putin. For exam- ple, he first denied that the ‘little green men’ in Crimea were Russian soldiers but later admitted that they were. Similarly, he at first denied any desire to see Crimea join Russia, but then he admitted that that had been his plan all along (Pifer 2015).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 18pt;text-indent: 0pt;text-align: left;">FINDINGS FROM PSYCHOLOGY: WHY RUSSIAN PROPAGANDA MIGHT WORK</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 18pt;text-indent: 0pt;text-align: justify;">Russia propaganda’s lack of commitment to objective reality or to consistency flies in the</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 11pt;padding-left: 58pt;text-indent: 0pt;text-align: justify;">face of the conventional wisdom on govern- ment persuasion, wisdom that holds credi- bility as paramount and infers credibility is lost through untruths or inconsistencies (US Department of Defense 2008; Paul 2011; Muñoz 2012). How can Russia’s approach to propaganda be inconsistent and often untrue but still persuasive? We turned to the relevant literature in psychology and social psychol- ogy to find out. In what follows, we review psychological findings relevant to each of the four distinctive characteristics of the Russian ‘firehose of falsehood’ propaganda model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 58pt;text-indent: 0pt;text-align: left;">HOW DO VOLUME AND DIVERSITY OF SOURCES CONTRIBUTE TO PERSUASIVENESS?</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 58pt;text-indent: 0pt;text-align: justify;">Russian propaganda involves dissemination of a high volume of messages across multiple sources, and decades of psychological research provides insights into the persuasive efficacy of this tactic. For example, experi- mental research has demonstrated the per- suasive advantage of multiple arguments presented by multiple sources over other conditions in which a single argument was presented by multiple sources and in which multiple arguments were presented by one source (Harkins &amp; Petty 1981). More recent research addressing the influence of cross media campaigns on consumer perceptions has also shown that presentation of informa- tion across more than one media type (e.g., television and the Internet) has a stronger effect on perceptions, attitudes, and inten- tions than presentation through one media type (Lim et al. 2015).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-top: 8pt;padding-left: 58pt;text-indent: 0pt;text-align: left;">HOW DO RAPIDITY AND REPETITION CONTRIBUTE TO PERSUASIVENESS?</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 58pt;text-indent: 0pt;text-align: justify;">Rapid dissemination of fabricated informa- tion provides Russian propaganda with a first</p><p style="padding-top: 11pt;padding-left: 18pt;text-indent: 0pt;text-align: justify;">mover advantage, thereby providing an early opportunity to frame how and whether people process subsequent information. Various studies have shown the competitive advan- tage that first movers can achieve, such as consumer preferences for first-in market brands over later brands (Kerin et al. 1992), preferences for options presented first during a serial order presentation (Hung et al. 2017), and a primacy processing effect where indi- vidual judgements favor messages presented first (Haugtvedt &amp; Wegener 1994).</p><p style="padding-left: 18pt;text-indent: 11pt;text-align: justify;">Although people tend to accept that first impressions matter, it is easy to underesti- mate the power of the first mover advantage. Some of that power comes from how humans store information. Our memories are not card catalogs in which we store individual facts in isolation. We store information in stories, in an integrated model of intertwined and inter- related bits of information that collectively frame our understanding of the world and support our worldview (Narvaez 1998; San Roque et al. 2012). When we receive a new factoid (something presented as fact, whether it is true or not) and we accept it, we do not simple store it in a cognitive card catalog. Rather, we integrate it into our understand- ing of the world. Therefore, when someone or something subsequently calls that factoid into question, we do not remove a single cognitive data card that holds that factoid. Instead, removing a factoid is a challenge to our existing story, and it is easier to continue to embrace a false impression than to change our understanding (Swire &amp; Ecker 2018).</p><p style="padding-left: 18pt;text-indent: 11pt;text-align: justify;">In addition to rapidity of dissemination, Russian propaganda’s use of repetition can also have powerful effects on attitudes and perceptions, such as by increasing familiar- ity with a message. Research suggests that stimuli or messages that match with one’s memories (e.g., are recognizable) are more positively evaluated than those that do not (Montoya et al. 2017). Through some rep- etition of messages, people can come to perceive the information to which they are repeatedly exposed as accurate and justified.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-top: 11pt;padding-left: 52pt;text-indent: 0pt;line-height: 106%;text-align: left;">HOW DOES A LACK OF COMMITMENT TO OBJECTIVE REALITY CONTRIBUTE TO PERSUASIVENESS?</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 52pt;text-indent: 0pt;line-height: 106%;text-align: justify;">Russian propaganda’s frequent use of false- hoods, or lack of commitment to objective reality, has the potential to be particularly persuasive. For example, research suggests that the more misinformation that people are presented with, the more difficulty they have in identifying misleading information (i.e., in differentiating true and false information; Pena et al. 2017). In addition, pathbreaking research by Vosoughi et al. (2018) examined the spread of verified true and verified false news stories from more than ten years of Twitter data. They found that false stories spread farther, faster, and deeper than true stories, with those effects being even more pronounced for false political news than for other categories. Their work confirms the old aphorism that lies are half-way around the world before the truth has its boots on. However, one should also consider when and why lies can be persuasive.</p><p style="padding-top: 1pt;padding-left: 52pt;text-indent: 11pt;line-height: 106%;text-align: justify;">Generally, sources and messages perceived as more credible are more persuasive than those seen as less credible (Pornpitakpan 2004), and ingroup members are perceived as more credible than individuals who belong to another group (Clark &amp; Maas 1988). De Dreu (2013) shows how humans are ‘parochial altruists’, willing to bear costs on behalf of groups to which they feel they belong and to fight, resist, or derogate rival outgroups. This creates considerable vulnerability to being persuaded by false messages that are propa- gated by members of a group or appear to have been propagated by members of that group.</p><p style="padding-left: 52pt;text-indent: 11pt;line-height: 106%;text-align: justify;">Group membership is one of many heu- ristics, or cognitive shortcuts, that people us to evaluate credibility. People use credibility heuristics in attempts to quickly determine source and message credibility. However, use of these heuristics can contribute to errors in credibility evaluations. As evidenced by their tactic of pretending to belong to another’s</p><p style="padding-top: 11pt;padding-left: 18pt;text-indent: 0pt;text-align: justify;">ingroup (either by claiming membership or by manifesting characteristics consistent with ingroup membership), Russian propa- gandists attempt to use these heuristics to their advantage.</p><p style="padding-left: 18pt;text-indent: 11pt;text-align: justify;">The endorsement heuristic is an addi- tional manipulatable cognitive shortcut used in the online environment, such that people are more likely to believe sources and mes- sages when others have done so (Metzger &amp; Flanigan 2013). Russian propagandists can influence this heuristic by spreading and sup- porting one another’s accounts, comments, and sites.</p><p style="padding-left: 18pt;text-indent: 11pt;text-align: justify;">Another heavily used heuristic is the self-confirmation heuristic, or a bias toward placing greater weight on individuals and messages that support pre-existing beliefs (Metzger &amp; Flanagin 2013). Whether a piece of information or news is consistent with our worldview is one of the first things we consider when evaluating its credibility and truthfulness (Lewandowsky et al. 2012). People search for and favor information con- sistent with their beliefs, also known as con- firmation bias, and they subject information inconsistent with their pre-existing beliefs to greater scrutiny, known as disconfirmation bias (Marsh &amp; Yang 2018). Our natural ten- dency toward confirmation bias is served in the contemporary information environment by ‘filter bubbles’ driven by our own choices about television programs and websites, and reinforced by algorithm-driven advertise- ments and search results (Pariser 2011).</p><p style="padding-left: 18pt;text-indent: 11pt;text-align: justify;">As evidenced by the limitations of heu- ristics, the potential persuasive efficacy of Russian propaganda might be bolstered due to human difficulties in differentiating truths from falsehoods. Demonstrating this diffi- culty, previous research found that participants relied on information from clearly fictional stories when subsequently responding to gen- eral knowledge questions, suggesting that people integrate incorrect information from untrue descriptions with their own understand- ing of the world (Marsh et al. 2003). Various additional studies have  also demonstrated</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 11pt;padding-left: 58pt;text-indent: 0pt;text-align: justify;">the impact that rumors, political misinforma- tion, and misleading media claims can have on individual beliefs (see Lewandowsky et al. 2012). Despite this difficulty in differentiation fact from fiction, people tend to overestimate their own ability to identify misleading infor- mation (Pena et al. 2017).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 58pt;text-indent: 0pt;text-align: left;">HOW DOES INCONSISTENCY AFFECT PERSUASIVENESS?</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 58pt;text-indent: 0pt;text-align: justify;">The fourth characteristic of Russian propa- ganda, its lack of consistency, runs counter to traditional wisdom regarding persuasion. Indeed, inconsistent messaging can hinder per- suasion, such that message recipients tend to more carefully  scrutinize inconsistent  mes- sages from a single source (Ziegler et al. 2004). At times, however, recipients may overlook inconsistencies. For example, when a source appears to have modified their messaging after greater consideration of different perspectives, recipient attitudinal confidence can increase (Rucker et al. 2008). Even if a source changes accounts, recipients are likely to evaluate the new message without overweighting the prior, ‘mistaken’, account, when the new message is sufficiently strong and the source is believed to be credible (Reich &amp; Tormala 2013).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 58pt;text-indent: 0pt;text-align: left;">DEFENDING AGAINST PROPAGANDA</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 58pt;text-indent: 0pt;text-align: justify;">Thus far, Russia’s disinformation campaign has been met with limited effective resistance (Lucas &amp; Nimmo 2015). What can Western governments, citizens, and companies do to protect themselves against Russian propaganda?</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 58pt;text-indent: 0pt;text-align: left;">EDUCATION</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 58pt;text-indent: 0pt;text-align: justify;">One of the most frequently proposed avenues to addressing the existence and potential</p><p style="padding-top: 11pt;padding-left: 18pt;text-indent: 0pt;text-align: justify;">influence of Russian propaganda and false- hoods is the promotion of media literacy. Media literacy education encourages people to use critical thinking when making deci- sions informed by media messages (Hobbs &amp; Jensen 2009). Media literacy education might include provision of information that increases knowledge regarding potential influencer goals and the tactics used to mis- lead and influence audiences, also known as persuasion knowledge (Friestad &amp; Wright 1994). Persuasion knowledge increases abili- ties to resist the influence of misleading claims and information (e.g., Xie &amp; Quintero Johnson 2015).</p><p style="padding-left: 18pt;text-indent: 11pt;text-align: justify;">Generally, media literacy interventions appear to have positive impacts on multiple outcomes, including knowledge and criti- cism of the media and awareness of media influence (Jeong et al. 2012). However, the full utility of media literacy education in promoting knowledge of and resistance to Russian propaganda across diverse audiences requires additional investigation. In addi- tion, the design of media literacy education efforts must be considered when promoting resistance to influence, such that it cannot be assumed that any media literacy educa- tion will be effective. For example, indi- viduals exposed to media literacy education might assume that they are already resistant to persuasion and propaganda, reducing the effectiveness of this education. Possessing knowledge of manipulation tactics and disin- formation does not guarantee that people will use this knowledge (Pratkanis &amp; Aronson 2001).</p><p style="padding-left: 18pt;text-indent: 11pt;text-align: justify;">People underestimate their own suscep- tibility to biases and misperceptions, such that they perceive they are less susceptible to biases in judgement and inferences than others, and this tendency might reduce the extent to which people pay attention to and use media literacy education. Pronin and col- leagues have termed this phenomenon ‘bias blind spot’, wherein people see themselves as less susceptible than others to multiple biases in cognition and motivation (Pronin,</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 11pt;padding-left: 52pt;text-indent: 0pt;text-align: justify;">2006; Pronin et al. 2002). This bias blind spot appears when people evaluate the perceived effects of fake news and online comments. Research has demonstrated that individuals believe that others, particularly those who are members of different social groups than themselves, are more susceptible to the harm- ful effects of fake news (Jang &amp; Kim 2018) and more influenced by online comments (Chen &amp; Ng 2016) than the individuals them- selves are. To be most effective, individuals need to be aware of, or made aware of, their personal vulnerability to be influenced by information, including deceptive and illegiti- mate messages (Sagarin et al. 2002).</p><p style="padding-left: 52pt;text-indent: 11pt;text-align: justify;">Through awareness of their own vulner- ability to influence, people may be more inclined to monitor their own responses to information and messages. For example, if someone experiences an emotional response to a message, their acknowledgement of these feelings might stimulate analytic con- siderations of why they have experienced these emotions, or what characteristics of the message primed these emotions (Pratkanis &amp; Aronson 2001).</p><p style="padding-left: 52pt;text-indent: 11pt;text-align: justify;">However, for people to remain vigilant to their own exposure and responses to false and misleading messages, they must have the cognitive resources to do so. Although they may have knowledge regarding different dis- information sources and tactics, individuals might not draw from this knowledge when fatigued. In other words, the ability to detect manipulative intent and falsehoods is dimin- ished when people are distracted, tired, or cognitively overloaded (Wentzel et al. 2010). Avoiding propaganda rich environments when fatigued, preoccupied, or in a similar cognitively vulnerable state may reduce the potential to be influenced by propaganda.</p><p style="padding-left: 52pt;text-indent: 11pt;text-align: justify;">Promoting analytic thinking and careful consideration of sources, messages, and one’s own cognitive biases and limitations can help individuals to develop a healthy skepticism to use when exposed to different pieces of information. However, ill-considered use of different strategies, such as by never trusting</p><p style="padding-top: 11pt;padding-left: 18pt;text-indent: 0pt;text-align: justify;">any new information, has the potential to lead to the development of unhealthy skepticism, or fear of and an unwillingness to consider new ideas (Johnson 2002).</p><p style="padding-left: 18pt;text-indent: 11pt;text-align: justify;">Certain practices might promote unhealthy skepticism. For example, as part of efforts to appear neutral, reporters often present claims from two or more sides of an issue or story without adjudicating this information, known as he said/she said reporting. The issue with this strategy is that it can provide credibility to clearly false claims and promote misper- ceptions (Weeks 2018). Although he said/ she said reporting might be implemented as part of journalistic efforts to appear unbiased, research suggests that journalistic interven- tion through provision of additional facts and analyses not only minimizes mispercep- tions but also promotes positive perceptions regarding news quality (Pingree et al. 2014).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 18pt;text-indent: 0pt;text-align: left;">DEBUNKING, REFUTING, COUNTERCLAIMS, AND ALTERNATIVE NARRATIVES</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 18pt;text-indent: 0pt;text-align: justify;">One tactic that has been used in attempts to counter falsehoods is to disseminate clear and credible corrections to this incorrect information, or to debunk the myths. The common, yet false, assumption about debunk- ing is that misperceptions are a function of a lack of knowledge, so simply conveying cor- rect information will be sufficient in elimi- nating the influence of the false information. This model, known as the ‘information defi- cit model’, is wrong (Cook &amp; Lewandowsky 2011). Corrections are often of limited use in reducing or eliminating reliance on misper- ceptions developed through exposure to falsehoods. Even if people receive and believe corrections, the previous falsehoods to which they were exposed, and had believed to be true, continue to impact their reasoning (Ecker et al. 2011). Successful debunking requires an understanding of not just what</p><p style="padding-left: 18pt;text-indent: 0pt;line-height: 16pt;text-align: justify;">people know and think, but <span class="s5">how </span>they think.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 11pt;padding-left: 58pt;text-indent: 11pt;text-align: justify;">Reliance on false information following a correction is thought to be due to mental models, or stories, that people develop after receiving initial information on a topic or event. A correction produces a gap in the mental model one had developed, and as noted previously, people would rather hold onto an incorrect mental model that contains falsehoods than an incomplete mental model that has gaps due to removed inaccuracies. To address this, corrections can include correct and factual alternative information to replace the incorrect information people held in their mental models (Swire &amp; Ecker 2018).</p><p style="padding-left: 70pt;text-indent: 0pt;line-height: 15pt;text-align: left;">In their <span class="s5">Debunking Handbook</span>, Cook and</p><p style="padding-left: 58pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">Lewandowsky (2011) propose that successful</p><p style="padding-left: 58pt;text-indent: 0pt;text-align: justify;">debunking efforts must have three major ele- ments: 1) a focus on core facts rather than the myth or falsehood being debunked in order to avoid reinforcing the familiarity of the false- hood; 2) preceding any explicit mention of the falsehood with forewarning that upcom- ing information is false; 3) an alternative explanation to replace the falsehood being debunked.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-top: 9pt;padding-left: 58pt;text-indent: 0pt;text-align: justify;">INOCULATION AND FOREWARNING</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 58pt;text-indent: 0pt;text-align: justify;">One way to address the potential influential effects of Russian propaganda and falsehoods is by inoculating audiences against these mes- sages or moving first. The concept underlying this approach is that, just as one’s immune system can be inoculated against viral infec- tions, so can one’s attitudes be inoculated against false and misleading information. Inoculation typically involves both forewarn- ing individuals about falsehoods to which they may be exposed and providing counter- arguments to these falsehoods. Multiple stud- ies have shown that inoculation promotes resistance to persuasion (Compton 2013). For inoculation to be most effective, audiences must understand that they will be the targets of persuasive attacks, acknowledge their</p><p style="padding-top: 11pt;padding-left: 18pt;text-indent: 0pt;text-align: justify;">potential vulnerabilities, have information that they can use to counterargue falsehoods to which they will be exposed, and be moti- vated to counterargue these falsehoods.</p><p style="padding-left: 18pt;text-indent: 11pt;text-align: justify;">Moving first can also be advantageous in addressing crises and negative informa- tion. Proactively communicating negative information about one’s organization or self is known as ‘stealing thunder’ (Pratkanis &amp; Aronson 2001). This approach allows those who implement it to control the information flow and minimize others’ ability to sensa- tionalize a topic. Further, stealing thunder can promote positive perceptions, such that the voluntary release of negative information promotes perceptions that the entity of inter- est is honest and credible. However, if audi- ences perceive this approach is being used to manipulate them, then the positive effects of stealing thunder disappear (Lee 2016).</p><p style="padding-left: 18pt;text-indent: 11pt;text-align: justify;">Naming and shaming, or discounting, the sources of and outlets for Russian propa- ganda and discussing Russian tactics are additional strategies for countering disinfor- mation that also assume a protective advan- tage can be gained by providing individuals with pertinent information. James Farwell (2018) advocates transparent discussion of Russian tactics and practices, both to increase public awareness of their efforts and to signal to the Russians that the United States is aware of and does not approve of these activities. He also advocates better enforcement of the requirement for various entities to register as foreign agents (in com- pliance with the Foreign Agents Registration Act) and would like to see Russia’s Sputnik International, RT, and other foreign agents required to label their informational materi- als (web pages, broadcasts, etc.) with a con- spicuous disclosure of their foreign agent status. This source identification strategy is most likely to effectively counter falsehoods when people must attribute information to a source or otherwise remember where information came from. However, people often forget the sources of information. As such, they may remember the influential</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 11pt;padding-left: 52pt;text-indent: 0pt;text-align: justify;">information and forget that this information came from a source with little or no credibil- ity (Marsh &amp; Yang, 2018).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-top: 9pt;padding-left: 52pt;text-indent: 0pt;text-align: justify;">FACT-CHECKING AND VERIFICATION</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 52pt;text-indent: 0pt;text-align: justify;">Consumers increasingly use social media sites and applications to get their news, and social media are major sources through which Russian propagandists can spread dis- information (Lazar et al. 2018). Keir Giles</p><p style="padding-left: 52pt;text-indent: 0pt;line-height: 15pt;text-align: justify;">(2017:2) at the <span class="s5">Council on Foreign Relations</span></p><p style="padding-left: 52pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">suggests that ‘Social media companies</p><p style="padding-left: 52pt;text-indent: 0pt;text-align: justify;">should more aggressively police their plat- forms for malicious state-sponsored content, and they should work with news organiza- tions to promote verified and fact-checked content on their platforms’. Anne Applebaum and her colleagues (2018) suggest broader revision to the digital rules, including a social media code of conduct, more transparency regarding political advertising, and better systems for authentication of users. Categories of services for addressing online disinformation include fact-checking and verification services (Brandtzaeg et al. 2017). Fact-checking services examine and ascer- tain the validity and credibility of online content, and verification services analyze the authenticity of users and pieces of online content (e.g., images).</p><p style="padding-left: 52pt;text-indent: 11pt;text-align: justify;">Three potential social media approaches to fact-checking are (i) increased use of human editors; (ii) crowdsourcing; (iii) techno- logical or algorithmic solutions (Althuis &amp; Strand 2018), and each of these approaches has its own inherent limitations. The vol- ume of online content hinders the ability for human editors and experts to review all, or even most, online claims, and crowdsourc- ing fact-checking can be both highly prone to error and resource intensive (Babakar 2018). Ensuring accuracy among computational fact-checkers can also be challenging, in part due to large variability in online content and the ability of disinformation disseminators</p><p style="padding-top: 11pt;padding-left: 18pt;text-indent: 0pt;text-align: justify;">to modify messaging content and strategies (Boididou et al. 2014).</p><p style="padding-left: 18pt;text-indent: 11pt;text-align: justify;">As discussed earlier, debunking previously believed claims can be ineffective. As such, consumers would ideally receive informa- tion regarding the validity and credibility of claims before they have a chance to believe them. This might be accomplished by encour- aging consumers to include information from credible fact-checking sites in the feed of information they receive (e.g., ‘follow’ fact- checking sites) and by labeling social media content.</p><p style="padding-left: 18pt;text-indent: 11pt;text-align: justify;">Traditionally, social media has sought to democratize the news, allowing that egre- gious political clickbait and items from respected news media appear without dis- crimination in newsfeeds. However, sources could be scored based on criteria that users value and contribute to trust, and those scores could be displayed (Waldrop 2017). Such a labeling approach must be implemented care- fully, however. A 2017 study by Pennycook and Rand found that a newsfeed in which some items were labeled as ‘disputed’ back- fired, in that all items that had no flag were then considered to be more credible. This suggests that, to function effectively, a labe- ling system would need to label all items, even if just with a placeholder tag that indi- cates an item is new and has not yet been either verified or disputed.</p><p style="padding-left: 18pt;text-indent: 11pt;text-align: justify;">Better verification of users and content through identity resolution and bot removal has also been considered as part of efforts to disinformation. In April of 2018, the European Commission announced the intro- duction of a European Union-wide code of practice on disinformation. Among the things that will be required of online platforms are transparency about sponsored content (partic- ularly political advertising) and taking meas- ures to identify and close fake accounts and accounts run by bots (European Commission 2018).</p><p style="padding-left: 18pt;text-indent: 11pt;text-align: justify;">Supporting increased use of regulations requiring user and content verification, James Farwell (2018) has noted that the freedom of</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 11pt;padding-left: 58pt;text-indent: 0pt;text-align: justify;">speech commonly guaranteed in democracies does not extend to robots, and inauthentic speech or speech artificially echoed should not be protected. However, there are risks involved in indiscriminate use of verification, particularly with regard to a user’s identity. While identity resolution and elimination of false personas (run by bots or otherwise) would be a positive step in protecting estab- lished democracies, it could be dangerous in fledgling or non-democracies. If discover- able by an authoritarian regime, it would not be a positive development for pro-democracy advocates to be restricted to one account per platform, each associated with a confirmed identity.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-top: 9pt;padding-left: 58pt;text-indent: 0pt;text-align: justify;">CHANGING THE INCENTIVES</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 58pt;text-indent: 0pt;text-align: justify;">Beyond fact-checking and verification ser- vices, sites and application can also reduce incentives for promoting misinformation. Some entities may use disinformation to bol- ster their ad revenues or brands, so making it harder to profit from disinformation may help to decrease its production and dissemi- nation. Addressing this, Waldrop (2017) noted positive steps by Facebook and Google in 2016 and 2017 to prevent blatantly fake news sites from earning money on their advertising networks and lowering the news- feed ranking of low-quality sources.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-top: 9pt;padding-left: 58pt;text-indent: 0pt;text-align: justify;">REGULATING CONTENT</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 58pt;text-indent: 0pt;text-align: justify;">Several countries have implemented regula- tions or policies aimed at reducing the flow of propaganda. For example, in 2014, Latvia suspended broadcasts from Russia’s RTR Planeta for three months based on a violation of their national law on Electronic Mass Media (Freedom House 2015). In addition, Indian authorities have warned social media group administrators that they can be held</p><p style="padding-top: 11pt;padding-left: 18pt;text-indent: 0pt;text-align: justify;">accountable for disseminating false news or fabricating stories that could inflame com- munal tension (Connolly et al. 2016).</p><p style="padding-left: 18pt;text-indent: 11pt;text-align: justify;">Notably, content regulation has the poten- tial to become, or might be considered to be, censorship that prevents the human right to freedom of expression. In addition, if a site or application removes content without edu- cating users regarding why or permitting appeals, then users may feel dehumanized and frustrated (Myers West 2018). This can lead users to search for alternative online communication avenues.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-top: 9pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">CYBER-BLURRING</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 18pt;text-indent: 0pt;text-align: justify;">An additional possible avenue for addressing Russian propagandists’ tactics is to create confusion through the use of cyber-blurring. Cyber-blurring includes creating fake email accounts and fake documents to confuse and slow hackers. This tactic appears to have been used as a counteroffensive measure employed by Emmanuel Macron’s campaign team during the 2017 French presidential election to address Russian hackers (Nossiter et al. 2017). Although this might be effective in addressing hacker activity, deliberate dis- semination of falsified information to the public could harm the credibility of an indi- vidual or organization.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-top: 9pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">CONCLUSION</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 18pt;text-indent: 0pt;text-align: justify;">In summary, Russia’s  approach to propa- ganda represents a firehose of falsehood with four distinctive features. It is high volume and multi-channel; rapid, continuous, and repeti- tive; shows no commitment to objective real- ity; and shows no commitment to consistency. Although difficult to quantify, at least some research suggests this approach has been effective in influencing audiences. This runs counter to conventional wisdom on</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 11pt;padding-left: 52pt;text-indent: 0pt;text-align: justify;">government persuasion, which suggests that truth and consistency are of paramount importance, However, research in psychology suggests these features can be persuasive.</p><p style="padding-left: 52pt;text-indent: 11pt;text-align: justify;">Various studies have demonstrated that dissemination of a high volume of messages across different types of media sources can influence attitudes and perceptions. In addi- tion, rapid dissemination of information provides a first mover advantage that can influence the mental models, or stories, that individuals create; thereby influencing an individual’s interpretation of related subse- quent information. Repetitive and continu- ous presentation of falsehoods makes it more difficult for audiences to identify misleading information, and propagandists can use false and misleading information about themselves (e.g., pretend to be in a group) and a topic to manipulate the cognitive shortcuts, or heu- ristics, that people employ. Although a lack of consistency can reduce persuasive impact, there are instances when a lack of consistency might promote persuasion, such as when a source that is believed to be credible appears to have been previously mistaken.</p><p style="padding-left: 52pt;text-indent: 11pt;text-align: justify;">Although research suggests that the char- acteristics of Russian propaganda might pro- mote its ability persuade audiences, there are avenues to countering this propaganda. For example:</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p class="s4" style="padding-left: 67pt;text-indent: -15pt;text-align: justify;">Media literacy programs that both increases awareness of personal vulnerabilities to being influenced and educate audiences regarding the goals and tactics of propagandists are avenues to reducing the persuasive efficacy of Russian propaganda.</p></li><li><p class="s4" style="padding-left: 67pt;text-indent: -15pt;text-align: justify;">Debunking efforts that not only indicate previous information to which audiences were exposed was false but also provide an alternative expla- nation to replace the incorrect information can also be used in efforts to counter disinformation.</p></li><li><p class="s4" style="padding-left: 67pt;text-indent: -15pt;text-align: justify;">Inoculation efforts that inform audiences that they will be targeted and might be vulnerable to persuasive attacks and that also provide audi- ences with information to counterargue false- hoods can be pursued to reduce audiences’ potential to be influenced.</p></li></ul><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l2"><li><p class="s4" style="padding-left: 33pt;text-indent: -15pt;text-align: justify;">Rapid and increased fact-checking of claims and verification of information can be used to reduce audiences’ exposure to disinformation on social media.</p></li><li><p class="s4" style="padding-left: 33pt;text-indent: -15pt;text-align: justify;">Reducing the potential for disinformation dis- seminators to profit from their messaging tactics may also help to reduce the creation and spread of disinformation.</p></li><li><p class="s4" style="padding-left: 33pt;text-indent: -15pt;text-align: justify;">Another avenue that different governments have considered or used is that of increased regulation of social media users and content. Importantly, the multiple implications of and potential issues with this approach, including the potential to limit freedom of expression, should be strongly considered.</p></li><li><p class="s4" style="padding-left: 33pt;text-indent: -15pt;text-align: justify;">New strategies to counter propagandists and disinformation, such as cyber-blurring, continue to be considered and developed, and their utility in reducing the effectiveness of Russian propa- ganda should be evaluated.</p></li></ul><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 18pt;text-indent: 0pt;text-align: justify;">Use of only one approach to countering dis- information is likely to be far less effective than a multi-pronged approach that promotes multiple different avenues. Further, any set of approaches that is pursued with little or no consideration of the social contexts and audience characteristics of those who will be exposed to these efforts will also be less effective. Continued development of new strategies, based in theory and research, fol- lowed by implementation and systematic evaluation is also needed to effectively coun- ter Russia propaganda.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-top: 9pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">REFERENCES</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 28pt;text-indent: -10pt;text-align: justify;">Althuis, Jente and Siri Strand, ‘Countering Fake News’, in Jente Althuis, and Leonie Haiden, eds., <i>Fake News: A Roadmap</i>, Riga: The NATO StratCom Centre of Excellence, 2018, pp. 68–77.</p><p class="s4" style="padding-left: 28pt;text-indent: -10pt;text-align: justify;">Andriukaitis, Lukas, ‘Russia’s “Nazi” Narrative Against Lithuania and the Baltic States’, <i>Integrity Initiatiave.net</i>, April 6, 2018.</p><p class="s4" style="padding-left: 28pt;text-indent: -10pt;text-align: justify;">Applebaum, Anne, Edward Lucas, Ben Nimmo, Martin Innes, Keir Giles, Louis Brooke, Tanya Bogdanova, Rebecca Wiles, and Peter</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 69pt;text-indent: 0pt;text-align: justify;">Pomerantsev, ‘Kremlin disinformation cam- paigns: Recommendations to counter Russia computational propaganda in the UK’, Arena, Institute for Global Affairs, London School of Economics, 2018.</p><p class="s4" style="padding-left: 69pt;text-indent: -10pt;text-align: justify;">Armstrong, Matthew, ‘Russia’s War on Infor- mation’, War on the Rocks, December 15, 2014.</p><p class="s4" style="padding-left: 69pt;text-indent: -10pt;text-align: justify;">Averin, Alexander, ‘Russia and Its Many Truths’, in Jente Althuis, and Leonie Haiden, eds., <i>Fake News: A Roadmap, </i>Riga: The NATO StratCom Centre of Excellence, 2018, pp. 59–67.</p><p class="s4" style="padding-left: 69pt;text-indent: -10pt;text-align: justify;">Babakar, Meva, ‘Can Crowdsourcing Scale Fact-Checking Up, Up, Up? Probably Not, and Here’s Why’, <i>Nieman Lab, </i><a href="http://www/" class="a" target="_blank">June 6, 2018. Last retrieved on July 30, 2018 from: </a>www. niemanlab.org/2018/06/can-crowdsourcing- scale-fact-checking-up-up-up-probably-not- and-heres-why/</p><p class="s4" style="padding-left: 69pt;text-indent: -10pt;text-align: justify;">Boididou, Christina, Symeon Papadopoulos, Yiannis Kompatsiaris, Steve Schifferes, and Nic Newman, ‘Challenges of Computational Verification in Social Multimedia’, Challenges of Computational Verification in Social Mul- timedia. In <i>Proceedings of the 23rd Interna- tional Conference on World Wide Web</i>, ACM, 2014, pp. 743–748.</p><p class="s4" style="padding-left: 69pt;text-indent: -10pt;text-align: justify;">Brandtzaeg, Petter Bae, Asbjørn Følstad and María Ángeles Chaparro Domínguez, ‘How Journalists and Social Media Users Perceive Online Fact-Checking and Verification Ser- vices’, <i>Journalism Practice</i>, Vol. 12(9), 2017,</p><p class="s4" style="padding-left: 69pt;text-indent: 0pt;text-align: justify;">pp. 1109–1129.</p><p class="s4" style="padding-left: 69pt;text-indent: -10pt;text-align: justify;">Brooking, Emerson T. and P.W. Singer, ‘War Goes Viral: How Social Media is Being Weap- onized Across the World’, <i>The Atlantic</i>, November 2016.</p><p class="s4" style="padding-left: 69pt;text-indent: -10pt;text-align: justify;">Chen, Adrian, ‘The Agency’, <i>New York Times Magazine</i>, June 2, 2015.</p><p class="s4" style="padding-left: 69pt;text-indent: -10pt;text-align: justify;">Chen, Gina Masullo and Yee Man Margaret Ng, ‘Third-Person Perception of Online Com- ments: Civil Ones Persuade You More Than Me’, <i>Computers in Human Behavior, </i>Vol. 55, 2016, pp. 736–742.</p><p class="s4" style="padding-left: 69pt;text-indent: -10pt;text-align: justify;">Clark, Russell D. and Anne Maas, ‘The Role of Categorization and Perceived Source Credi- bility in Minority Influence’, <i>European Jour- nal of Social Psychology, </i>Vol. 18, 1988, pp. 381–394.</p><p class="s4" style="padding-left: 69pt;text-indent: -10pt;text-align: justify;">Connolly, Kate, Angelique Chrisafis, Poppy McPherson, Stephanie Kirchgaessner, Benja- min Haas, Dominic Phillips, Elle Hunt, and</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 29pt;text-indent: 0pt;text-align: justify;">Michael Safi. ‘Fake News: An Insidious Trend That’s Fast Becoming a Global Problem’. <i>The Guardian</i>, December 2, 2016. www.the- guardian.com/media/2016/dec/02/ fake-news-facebook-us-election-around- the-world.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Cook, John and Stephan Lewandowsky, <i>The Debunking Handbook</i>, St. Lucia, Australia: University of Queensland, 2011.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Compton, Joshua, ‘Inoculation Theory’, in James P. Dillard and Lijang Shen, eds., <i>The Sage Handbook of Persuasion: Develop- ments in Theory and Practice</i>, 2nd edition, Washington DC: Sage, 2013, pp. 220–236.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Davis, Julia, ‘Russia’s Top 100 Lies About Ukraine’, <i>The Examiner</i>, August 11, 2014.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">De Dreu Carsten, ‘Human Cooperation: Chal- lenges for Science and Practice’, <i>Psychologi- cal Science in the Public Interest</i>, Vol. 14(3), 2013, pp. 117–118.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Disinformation, ‘Weekly Disinformation Review’, <i>Disinfo</i>, January 14, 2016.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Ecker, Ullrich K.H., Stephan Lewandowsky, Briony Swire, and Darren Chang, ‘Correcting False Information in Memory: Manipulating the Strength of Misinformation Encoding and Its Retraction’, <i>Psychonomic Bulletin &amp; Review, </i>Vol. 18(3), 2011, pp. 570–578.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;"><a href="http://europa/" class="a" target="_blank">European Commission, ‘Tackling Online Disin- formation: Commission Proposes an EU- wide Code of Practice’, Brussels, Belgium: Press Release, April 26, 2018. </a>http://europa. eu/rapid/press-release_IP-18-3370_en.htm</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Farwell, James, P., ‘Countering Russian Med- dling in U.S. Political Processes’, <i>Parameters</i>, Vol. 48, No. 1, Spring 2018, pp. 37-47.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Freedom House, ‘Freedom on the Net 2017: Manipulating Social Media to Undermine Democracy’, accessed May 9, 2018, https:// freedomhouse.org/report/freedom-net/ freedom-net-2017.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Freedom House, Freedom of the Press, ‘Lativia’, website, 2015. https://freedomhouse.org/ report/freedom-press/2015/latvia</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Friestad, Marian and Peter Wright, ‘The Persua- sion Knowledge Model: How People Cope with Persuasion Attempts’, <i>Journal of Con- sumer Research, </i>Vol. 21, 1994, pp. 1–31.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Giles, Keir, ‘Countering Russian Information Operations in the Age of Social Media’, <i>Council on Foreign Relations</i>, November 21, 2017.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 63pt;text-indent: -10pt;text-align: justify;">Goble, Paul A., ‘Top 10 Fakes of Russian Propa- ganda About Ukraine in 2015’, <i>Euromaidan Press</i>, December 26, 2015.</p><p class="s4" style="padding-left: 63pt;text-indent: -10pt;text-align: justify;">Haugtvedt, Curtis P. and Duane T. Wegener, ‘Message Order Effects on Persuasion: An Attitude Strength Perspective’, <i>Journal of Consumer Research, </i>Vol. 21(2), 1994,</p><p class="s4" style="padding-left: 63pt;text-indent: 0pt;text-align: justify;">pp. 205–218.</p><p class="s4" style="padding-left: 63pt;text-indent: -10pt;text-align: justify;">Harkins, Stephen and Richard E. Petty, ‘Effect of Source Magnification of Cognitive Effort on Attitudes: An Information-Processing View’, <i>Journal of Personality and Social Psy- chology, </i>Vol. 40(3), 1981, pp. 401–413.</p><p class="s4" style="padding-left: 63pt;text-indent: -10pt;text-align: justify;">Hobbs, Renee and Amy Jensen, ‘The Past, Pre- sent, and Future of Media Literacy Educa- tion’, <i>Journal of Media Literacy Education, </i>Vol. 1, 2009, pp. 1–11.</p><p class="s4" style="padding-left: 63pt;text-indent: -10pt;text-align: justify;">Hung, Yu-Chen, Liang Song, Chih-Wei (Fred) Chao, and Chong Guan, ‘Love at First Sight: The Effect of Presentation Order on Evalua- tion of Experiential Options in Luxury Tour Packages’, <i>Journal of Business Research</i>, Vol. 81, 2017, pp. 181–191.</p><p class="s4" style="padding-left: 52pt;text-indent: 0pt;text-align: right;">Jang, S. Mo and Joon K. Kim, ‘Third Person Effects of Fake News: Fake News Regulation and Media Literacy Interventions’, <i>Computers in Human Behavior, </i>Vol. 80, 2018, pp. 295–302. Jeong, Se-Hoon, Hyunyi Cho, and Yoori Hwang, ‘Media Literacy Interventions: A Meta-Analytic Review’, <i>Journal of Communi-</i></p><p class="s6" style="padding-left: 63pt;text-indent: 0pt;text-align: justify;">cation, <span class="s4">Vol. 62, 2012, pp. 454–472.</span></p><p class="s4" style="padding-left: 63pt;text-indent: -10pt;text-align: justify;">Johnson, Marcia K., ‘Reality Monitoring’, <i>APS Observer, </i>2002.</p><p class="s4" style="padding-left: 52pt;text-indent: 0pt;text-align: justify;">Kerin, Roger A., Rajan Varadarajan, and Robert</p><p class="s4" style="padding-left: 63pt;text-indent: 0pt;text-align: justify;">A. Peterson, ‘First-Mover Advantage: A Syn- theses, Conceptual Framework, and Research Propositions’, <i>Journal of Marketing, </i>Vol. 56, 1992, pp. 33–52.</p><p class="s4" style="padding-left: 63pt;text-indent: -10pt;text-align: justify;">Lazar, David M. J., Matthew A. Baum, Yochai Benkler, Adam J. Berinsky, Kelly M. Greenhill, Filippo Menczer, Miriam J. Metzger, Brendan Nyhan, Gordon Pennycook, David Roths- child, Michael Schudson, Steven A. Sloman, Cass R. Sunstein, Emily A. Thorson, Duncan</p><p class="s4" style="padding-left: 63pt;text-indent: 0pt;text-align: justify;">J. Watts, and Jonathan L. Zittrain, ‘The Sci- ence of Fake News: Addressing Fake News Requires a Multidisciplinary Effort’, <i>Science, </i>Vol. 359, 2018, pp. 1094–1096.</p><p class="s4" style="padding-left: 63pt;text-indent: -10pt;text-align: justify;">Lee, Sang Yeal, ‘Weathering the Crisis: Effects of Stealing Thunder in Crisis Communication’, <i>Public Relations Review, </i>Vol. 42, 2016,</p><p class="s4" style="padding-left: 63pt;text-indent: 0pt;text-align: justify;">pp. 336–344.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 18pt;text-indent: 0pt;text-align: justify;">Lelich, Milan, ‘Victims of Russian Propaganda’,</p><p class="s6" style="padding-left: 18pt;text-indent: 10pt;text-align: justify;">NewEastern Europe<span class="s4">, July 25, 2014 Lewandowsky, Stephan, Ullrich K. H. Ecker, Col-</span></p><p class="s4" style="padding-left: 29pt;text-indent: 0pt;text-align: justify;">leen M. Seifert, Norbert Schwarz, and John Cook, ‘Misinformation and Its Correction: Continued Influence and Successful Debias- ing’, <i>Psychological Science in the Public Inter- est, </i>Vol. 13(3), 2012, pp. 106–131.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Lim, Joon Soo, Sung Yoon Ri, Beth Donnelly Egan, and Frank A. Biocca, ‘The Cross-Plat- form Synergies of Digital Video Advertising: Implications for Cross-Media Campaigns in Television, Internet, and Mobile TV’, <i>Com- puters in Human Behavior</i>, Vol. 48, 2015, pp. 463–472.</p><p class="s4" style="padding-left: 12pt;text-indent: 0pt;text-align: right;">Lucas, Edward, ‘Russia Has Published Books I Didn’t Write!’ <i>Daily Beast</i>, August 20, 2015. Lucas, Edward and Ben Nimmo, ‘Information Warfare: What Is It and How to Win It?’ <i>Center for European Policy Analysis</i>, Paper</p><p class="s4" style="padding-left: 29pt;text-indent: 0pt;text-align: justify;">No. 1, November 2015.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Marsh, Elizabeth J., Michelle L. Mead, and Henry L. ‘Roediger III, Learnings Facts from Fiction’, <i>Journal of Memory and Language, </i>Vol. 49, 2003, pp. 519–536.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Marsh, Elizabeth J. and Brenda W. Yang, ‘Believing Things That Are Not True: A Cog- nitive Science Perspective on Misinforma- tion’, in Brian G. Southwell, Emily A. Thorson, and Laura Sheble, eds., <i>Misinformation and Mass Audiences, </i>Austin, TX: University of Texas Press, 2018, pp. 15–34.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">McGeehan, Timothy, P., ‘Countering Russian Disinformation’, <i>Parameters</i>, Vol. 48, No. 1,</p><p class="s4" style="padding-left: 29pt;text-indent: 0pt;text-align: justify;">Spring 2018.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Metzger, Miriam J. and Andrew J. Flanagin, ‘Credibility and Trust of Information in Online Environments: The Use of Cognitive Heuris- tics’, <i>Journal of Pragmatics, </i>Vol. 59, 2013, pp. 210–220.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Miller, James, ‘Russian Media: Conspiracy The- ories and Reading Comprehension Issues’, <i>The Interpreter</i>, September 18, 2013.</p><p class="s4" style="padding-left: 18pt;text-indent: 0pt;text-align: justify;">Montoya, R. Matthew, Robert S. Horton, Jack</p><p class="s4" style="padding-left: 29pt;text-indent: 0pt;text-align: justify;">L. Vevea, Martyna Citkowicz, and Elissa A. Lauber, ‘A Re-Examination of the Mere Expo- sure Effect: The Influence of Repeated Expo- sure on Recognition, Familiarity, and Liking’, <i>Psychological Bulletin, </i>2017, Vol. 143(5), pp. 459–498.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Muñoz, Arturo, U.S. Military Information Oper- ations in Afghanistan: Effectiveness of</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 69pt;text-indent: 0pt;text-align: justify;">Psychological Operations 2001–2010, Santa Monica, CA: RAND Corporation, MG-1060, 2012.</p><p class="s4" style="padding-left: 69pt;text-indent: -10pt;text-align: justify;">Myers West, Sarah, ‘Censored, Suspended, Shadowbanned: User Interpretations of Con- tent Moderation on Social Media Platforms’, <i>New Media &amp; Society, </i>2018, pp. 1–18.</p><p class="s4" style="padding-left: 69pt;text-indent: -10pt;text-align: justify;">Narvaez, Darcia, ‘The Influence of Moral Sche- mas on the Reconstruction of Moral Narra- tives in Eighth Graders and College Students’, <i>Journal of Educational Psychology</i>, Vol. 90(1), 1998, pp. 13–24.</p><p class="s4" style="padding-left: 69pt;text-indent: -10pt;text-align: justify;">Nossiter, Adam, David, E. Sanger, and Nicole Perlroth, ‘Hackers Came, But the French Were Prepared’, <i>The New York Times</i>, May 9, 2017.</p><p class="s4" style="padding-left: 69pt;text-indent: -10pt;text-align: justify;">Oliker, Olga, ‘Russia’s New Military Doctrine: Same as the Old Doctrine, Mostly’, <i>Washing- ton Post</i>, January 15, 2015.</p><p class="s4" style="padding-left: 52pt;text-indent: 0pt;text-align: right;">Pariser, Eli, The Filter Bubble: What the Internet is Hiding From You, London: Penguin, 2011. Paul, Christopher, <i>Strategic Communication: Origins, Concepts, and Current Debates</i>, Santa Barbara, CA: Praeger Security Interna-</p><p class="s4" style="padding-left: 69pt;text-indent: 0pt;text-align: justify;">tional, 2011.</p><p class="s4" style="padding-left: 69pt;text-indent: -10pt;text-align: justify;">Paul, Christopher and Miriam Matthews, <i>The Russian ‘Firehose of Falsehood’ Propaganda Model: Why It Might Work and Options to Counter It</i><a href="http://www.rand.org/pubs/" class="a" target="_blank">. Santa Monica, CA: RAND Corpo- ration, 2016. </a>https://www.rand.org/pubs/ perspectives/PE198.html.</p><p class="s4" style="padding-left: 69pt;text-indent: -10pt;text-align: justify;">Paul, Christopher, Jessica Yeats, Colin P. Clarke, Miriam Matthews, and Lauren Skrabala, <i>Assessing and Evaluating Department of Defense Efforts to Inform, Influence, and Per- suade: Handbook for Practitioners</i>, Santa Monica, CA: RAND Corporation, RR-809/ 2-OSD,2015.As of May 15, 2018: www.rand. org/pubs/research_reports/RR809z2.html</p><p class="s4" style="padding-left: 69pt;text-indent: -10pt;text-align: justify;">Pena, Michelle M., J. Zoe Klemfuss, Elizabeth F. Loftus, and Amelia Mindthoff, ‘The Effects of Exposure to Differing Amounts of Misinfor- mation and Source Credibility Perception on Source Monitoring and Memory Accuracy’, <i>Psychology of Consciousness: Theory, Research, and Practice</i>, Vol. 4(4), 2017,</p><p class="s4" style="padding-left: 69pt;text-indent: 0pt;text-align: justify;">pp. 337–347.</p><p class="s4" style="padding-left: 69pt;text-indent: -10pt;text-align: justify;">Pennycook, Gordon and Rand, David G. ‘The Implied Truth Effect: Attaching Warnings to a Subset of Fake News Stories Increases Perceived Accuracy of Stories Without Warnings’ (December 8, 2017). Available</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 29pt;text-indent: 0pt;text-align: justify;"><a href="http://dx.doi.org/10.2139/ssrn" class="a" target="_blank">at SSRN: </a>http://dx.doi.org/10.2139/ssrn. 3035384</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Peisakhin, Leonid and Arturas Rozenas, ‘Elec- toral Effects of Biased Media: Russian Televi- sion in Ukraine’, <i>American Journal of Political Science</i>, March 30, 2018, pp. 1–16.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Pifer, Steven, ‘Putin, Lies and His “Little Green Men”’, CNN, March 20, 2015.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Pingree, Raymond James, Dominique Brossard, and Douglas M. McLeod, ‘Effects of Journal- istic Adjudication on Factual Beliefs, News Evaluations, Information Seeking, and Epis- temic Political Efficacy’, <i>Mass Communication and Society, </i>Vol. 17, 2014, pp. 615–638.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Pomerantsev, Peter and Michael Weiss, The Menace of Unreality: How the Kremlin Weaponizes Information, Culture and Money, New York: Institute of Modern Russia and The Interpreter, 2014.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Pornpitakpan, Chanthika, ‘The Persuasiveness of Source Credibility: A Critical Review of Five Decades’ Evidence’, <i>Journal of Applied Social Psychology</i>, Vol. 34(2), 2004, pp. 243–281.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Pratkanis, Anthony and Elliot Aronson, <i>Age of Propaganda: The Everyday Use and Abuse of Persuasion, </i>New York: W.H. Freeman and Company, 2001.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Pronin, Emily, ‘Perception and Misperception of Bias in Human Judgment’, <i>TRENDS in Cogni- tive Science, </i>Vol. 11(1), 37–43, 2006.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Pronin, Emily, Daniel Y. Lin, and Lee Ross, ‘The Bias Blind Spot: Perceptions of Bias in Self Versus Others’, <i>Personality and Social Psychol- ogy Bulletin, </i>Vol. 28(3), 2002, pp. 369–381.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Reich, Taly and Zakary Tormala, ‘When Contra- dictions Foster Persuasion: An Attributional Perspective’, <i>Journal of Experimental Social Psychology</i>, Vol. 49(3), 2013, pp. 426–439</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Rucker, Derek D., Richard E. Petty, and Pablo Briñol, ‘What’s in a Frame Anyway? A Meta- Cognitive Analysis of the Impact of One Versus Two Sided Message Framing on Atti- tude Certainty’, <i>Journal of Consumer Psy- chology, </i>Vol. 18(2), 2008, pp. 137–149.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Sagarin, Brad J., Robert B. Cialdini, William Rice, and Sherman B. Serna, ‘Dispelling the Illusion of Invulnerability: The Motivations and Mechanisms of Resistance to Persua- sion’, <i>Journal of Personality and Social Psy- chology, </i>Vol. 83(3), 2002, pp. 526–541.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">San Roque, Lila, Alan Rumsey, Lauren Gawne, Stef Spronck, Darja Hoenigman, Alice</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 63pt;text-indent: 0pt;text-align: justify;">Carroll, Julie Colleen Miller, and Nicholas Evans, ‘Getting the Story Straight: Language Fieldwork Using a Narrative Problem-Solving Task’, <i>Language Documentation &amp; Conserva- tion</i>, Vol. 6, 2012, pp. 135–174.</p><p class="s4" style="padding-left: 63pt;text-indent: -10pt;text-align: justify;">Smith, Oli, ‘Watch: Russia’s Fake Ukraine War Report Exposed in Putin PR Disaster’, <i>Express</i>, August 24, 2015</p><p class="s4" style="padding-left: 63pt;text-indent: -10pt;text-align: justify;">Snyder, Timothy, <i>The Road to Unfreedom: Russia, Europe, America</i>, London: Tim Duggan Books, 2018.</p><p class="s4" style="padding-left: 63pt;text-indent: -10pt;text-align: justify;">Swire, Briony and Ullrich Ecker, ‘Misinformation and Its Correction: Cognitive Mechanisms and Recommendation’s for Mass Communi- cations’, in in Brian G. Southwell, Emily A. Thorson, and Laura Sheble, eds., <i>Misinforma- tion and Mass Audiences, </i>Austin, TX: Univer- sity of Texas Press, 2018, pp. 195–211.</p><p class="s4" style="padding-left: 63pt;text-indent: -10pt;text-align: justify;">Thomas, Timothy, ‘Russia’s 21st Century Infor- mation War: Working to Undermine and Destabilise Populations’, <i>Defence Strategic Communications </i>1, 2015.</p><p class="s4" style="padding-left: 63pt;text-indent: -10pt;text-align: justify;">US Department of Defense, Defense Science Board, <i>Report of the Defense Science Board Task Force on Strategic Communication</i>, Washington, DC, January 2008.</p><p class="s4" style="padding-left: 63pt;text-indent: -10pt;text-align: justify;">US District Court for the District of Columbia, Case 1:18-cr-0032-DFT, Criminal No. 18</p><p class="s4" style="padding-left: 63pt;text-indent: 0pt;text-align: justify;">U.S.C. §§ 2,371,1349,1028A, Filed February 16, 2018.</p><p class="s4" style="padding-left: 63pt;text-indent: -10pt;text-align: justify;">Volchek, Dmitry and Daisy Sindelar, ‘One Pro- fessional Russian Troll Tells All’, Radio Free Europe/Radio Liberty, March 25, 2015.</p><p class="s4" style="padding-left: 63pt;text-indent: -10pt;text-align: justify;">Vosoughi, Soroush, Deb Roy, and Sinan Aral, ‘The Spread of True and False News Online’,</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-left: 29pt;text-indent: 0pt;text-align: justify;">Social Science<span class="s4">, Vol. 359, March 9, 2018,</span></p><p class="s4" style="padding-left: 29pt;text-indent: 0pt;text-align: justify;">pp. 1146–1151.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Waldrop, Mitchell, ‘News Feature: The Genuine Problem of Fake News’, <i>Proceedings of the National Academy of Sciences of the United States of America</i>, Vol. 114(48), 2017,</p><p class="s4" style="padding-left: 29pt;text-indent: 0pt;text-align: justify;">pp. 12631–12634.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Wardle, Claire, ‘Fake news. It’s complicated’, <i>First Draft</i>, February 16, 2017. https://medium. com/1st-draft/fake-news-its-complicated- d0f773766c79</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Weeks, Brian, ‘Media and Political Mispercep- tions’. in Brian G. Southwell, Emily A. Thor- son, and Laura Sheble. eds., <i>Misinformation and Mass Audiences, </i>Austin, TX: University of Texas Press, 2018, pp. 140–156.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Wentzel, Daniel, Torsten Tomczak, and Andreas Herrmann, ‘The Moderating Effect of Manipulative Intent and Cognitive Resources on the Evaluation Narrative Ads’, <i>Psychology &amp; Marketing, </i>Vol. 27(5), 2010,</p><p class="s4" style="padding-left: 29pt;text-indent: 0pt;text-align: justify;">pp. 510–530.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Xie, Guang-Xin and Jessie M. Quintero John- son, ‘Examining the Third-Person Effect of Baseline Omission in Numerical Comparison: The Role of Consumer Persuasion Knowl- edge’, <i>Psychology &amp; Marketing, </i>Vol. 32(4), 2015, pp. 438–449.</p><p class="s4" style="padding-left: 29pt;text-indent: -10pt;text-align: justify;">Ziegler, Rene, Michael Diehl, Raffael Zigon, and Torsten Fett, ‘Source Consistency, Distinctiveness, and Consensus: The Three Dimensions of the Kelley ANOVA Model of Persuasion’, <i>Personality and Social Psychology Bulletin, </i>Vol. 30(3), 2004,</p><p class="s4" style="padding-left: 29pt;text-indent: 0pt;text-align: left;">352–364.</p></body></html>
